{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fadeb27",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9fadeb27",
    "outputId": "ef296101-d73c-4bb8-f590-053add6a1e58"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytorch_msssim'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpytorch_msssim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ms_ssim\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mskimage\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m structural_similarity \u001b[38;5;28;01mas\u001b[39;00m ssim\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mskimage\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m peak_signal_noise_ratio \u001b[38;5;28;01mas\u001b[39;00m psnr\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pytorch_msssim'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "#!{sys.executable} -m pip install lpips pytorch-msssim\n",
    "\n",
    "\n",
    "import torch                                 #The main library for deep learning\n",
    "import torch.nn as nn                        #Neural Network module in PyTorch\n",
    "from torch.nn import AdaptiveAvgPool2d\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim                  #contains optimizers for training neural networks(SGD,Adam)\n",
    "from torchvision import datasets             #torchvision is PyTorch library for vision tasks tandard datasets (MNIST, CIFAR10, ImageNet...) ,\n",
    "from torchvision import datasets, transforms #Preprocessing (resize, normalize, convert to tensor, etc.)\n",
    "from torchvision.models import inception_v3\n",
    "from torch.utils.data import DataLoader      #Loads dataset in batches and allows shuffling\n",
    "from torch.utils.data import Subset          #selects a subset of the dataset, useful for debugging or small experiments.\n",
    "from scipy import linalg\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pytorch_msssim import ms_ssim\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "import lpips\n",
    "import scipy.linalg\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2c940e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ec2c940e",
    "outputId": "efa31f1e-aa64-4986-e229-f6b21994dd33"
   },
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# Config\n",
    "# --------------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "image_number = 100      # Use more images for better training\n",
    "batch_size = 8\n",
    "epochs = 1\n",
    "lr = 0.0002\n",
    "img_size = 256\n",
    "lambda_G = 1            # Weight for adversarial loss\n",
    "lambda_L1 = 100         # Weight for L1 reconstruction loss\n",
    "GlobalFlag = False\n",
    "save_interval = 10      # Save checkpoint every N epochs\n",
    "\n",
    "# Training stability settings\n",
    "use_label_smoothing = True      # Smooth labels to prevent discriminator dominance\n",
    "real_label_value = 0.9          # Instead of 1.0\n",
    "fake_label_value = 0.1          # Instead of 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5505fc84",
   "metadata": {
    "id": "5505fc84"
   },
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# Dataset (MNIST [28x28] → 256x256x1) - Single channel for MRI-to-PET simulation\n",
    "# --------------------\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),      # [256, 256]\n",
    "    transforms.ToTensor(),                         # [1, 256, 256], values 0–1\n",
    "    transforms.Normalize((0.5,), (0.5,))          # [1, 256, 256], values -1 to 1\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4f8174",
   "metadata": {
    "id": "2d4f8174"
   },
   "outputs": [],
   "source": [
    "\n",
    "# ----------------\n",
    "# Load full MNIST datasets\n",
    "# ----------------\n",
    "train_data = datasets.MNIST(\"./data\", train=True, download=True, transform=transform)  # Each item is a tuple (image, label)\n",
    "val_data   = datasets.MNIST(\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "# ----------------\n",
    "# Create small subset (first 10 images) for both train and val\n",
    "# ----------------\n",
    "small_train_data = Subset(train_data, range(image_number))\n",
    "small_val_data   = Subset(val_data, range(image_number))\n",
    "\n",
    "# ----------------\n",
    "# DataLoaders\n",
    "# ----------------\n",
    "\n",
    "train_loader = DataLoader(small_train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(small_val_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc02bb8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461
    },
    "id": "ebc02bb8",
    "outputId": "528d3842-2047-4e0b-a067-a8feafbd3e6d"
   },
   "outputs": [],
   "source": [
    "def ShowSample():\n",
    "    \"\"\"Display a sample image from the dataset\"\"\"\n",
    "    img, label = train_data[1]  # [1, 256, 256]\n",
    "    print(\"Image shape:\", img.shape)\n",
    "\n",
    "    # Convert to [H, W] for grayscale plotting\n",
    "    img_np = img.squeeze().numpy()  # Remove channel dimension\n",
    "    img_np = (img_np + 1) / 2  # Denormalize [-1,1] → [0,1]\n",
    "\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(img_np, cmap='gray')\n",
    "    plt.title(f\"Label: {label}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "ShowSample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5b6844",
   "metadata": {
    "id": "7c5b6844"
   },
   "outputs": [],
   "source": [
    "\n",
    "# --------------------\n",
    "# Dense Block\n",
    "# --------------------\n",
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self, in_channels, growth_rate=32, n_layers=4, dropout=False):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.dropout = dropout\n",
    "        self.flag = GlobalFlag\n",
    "        for i in range(n_layers):\n",
    "            self.layers.append(nn.Conv2d(\n",
    "                in_channels + i * growth_rate, growth_rate, kernel_size=3, padding=1))\n",
    "            self.layers.append(nn.InstanceNorm2d(growth_rate))\n",
    "            self.layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            if dropout:\n",
    "                self.layers.append(nn.Dropout(0.5))\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = [x]\n",
    "        if self.flag:\n",
    "            logging.info(f\"DenseBlock input: {x.shape}\")\n",
    "        for i in range(0, len(self.layers), 4 if self.dropout else 3):\n",
    "            out = self.layers[i](torch.cat(features, dim=1))\n",
    "            out = self.layers[i+1](out)\n",
    "            out = self.layers[i+2](out)\n",
    "            if self.dropout:\n",
    "                out = self.layers[i+3](out)\n",
    "            features.append(out)\n",
    "        return torch.cat(features, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf51b72",
   "metadata": {
    "id": "9cf51b72"
   },
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# Encoder Blocks\n",
    "# --------------------\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, growth_rate=32, dropout=False):\n",
    "        super().__init__()\n",
    "        self.flag = GlobalFlag\n",
    "        self.dense = DenseBlock(in_channels, growth_rate, n_layers=3, dropout=dropout)\n",
    "        # Reduce channels to out_channels after dense concatenation\n",
    "        self.conv1x1 = nn.Conv2d(in_channels + 3*growth_rate, out_channels, kernel_size=1)\n",
    "        self.downsample = nn.Conv2d(out_channels, out_channels, 4, 2, 1)\n",
    "        self.relu = nn.LeakyReLU(0.2, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dense(x)\n",
    "        x = self.conv1x1(x)\n",
    "        x = self.downsample(x)\n",
    "        if self.flag:\n",
    "            logging.info(f\"EncoderBlock input: {x.shape}\")\n",
    "        return self.relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f34a607",
   "metadata": {
    "id": "7f34a607"
   },
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# Decoder Blocks\n",
    "# --------------------\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, growth_rate=32, dropout=False):\n",
    "        super().__init__()\n",
    "        self.flag = GlobalFlag\n",
    "        self.up = nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1)\n",
    "        self.dense = DenseBlock(out_channels, growth_rate, n_layers=3, dropout=dropout)\n",
    "        self.conv1x1 = nn.Conv2d(out_channels + 3*growth_rate, out_channels, kernel_size=1)\n",
    "        self.relu = nn.LeakyReLU(0.2, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.up(x)\n",
    "        x = self.dense(x)\n",
    "        x = self.conv1x1(x)\n",
    "        if self.flag:\n",
    "            logging.info(f\"DecoderBlock input: {x.shape}\")\n",
    "        return self.relu(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f1bf96",
   "metadata": {
    "id": "e1f1bf96"
   },
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# Dense-UNet Generator (U-Net with DenseBlocks)\n",
    "# --------------------\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1):\n",
    "        \"\"\"\n",
    "        Generator for MRI-to-PET synthesis\n",
    "        Args:\n",
    "            in_channels: 1 for grayscale MRI\n",
    "            out_channels: 1 for grayscale PET\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.flag = GlobalFlag\n",
    "\n",
    "        # Encoder\n",
    "        self.e1 = EncoderBlock(in_channels, 64)\n",
    "        self.e2 = EncoderBlock(64, 128)\n",
    "        self.e3 = EncoderBlock(128, 256)\n",
    "        self.e4 = EncoderBlock(256, 512)\n",
    "        self.e5 = EncoderBlock(512, 512)\n",
    "        self.e6 = EncoderBlock(512, 512)\n",
    "        self.e7 = EncoderBlock(512, 512, dropout=True)\n",
    "\n",
    "        # Decoder with skip connections\n",
    "        self.d1 = DecoderBlock(512, 512, dropout=True)\n",
    "        self.d2 = DecoderBlock(512+512, 512, dropout=True)\n",
    "        self.d3 = DecoderBlock(512+512, 512, dropout=True)\n",
    "        self.d4 = DecoderBlock(512+512, 256)\n",
    "        self.d5 = DecoderBlock(256+256, 128)\n",
    "        self.d6 = DecoderBlock(128+128, 64)\n",
    "\n",
    "        # Final output layer\n",
    "        self.final = nn.ConvTranspose2d(64+64, out_channels, 4, 2, 1)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder path\n",
    "        e1 = self.e1(x)\n",
    "        e2 = self.e2(e1)\n",
    "        e3 = self.e3(e2)\n",
    "        e4 = self.e4(e3)\n",
    "        e5 = self.e5(e4)\n",
    "        e6 = self.e6(e5)\n",
    "        e7 = self.e7(e6)\n",
    "\n",
    "        # Decoder path with skip connections\n",
    "        d1 = self.d1(e7)\n",
    "        d2 = self.d2(torch.cat([d1, e6], 1))\n",
    "        d3 = self.d3(torch.cat([d2, e5], 1))\n",
    "        d4 = self.d4(torch.cat([d3, e4], 1))\n",
    "        d5 = self.d5(torch.cat([d4, e3], 1))\n",
    "        d6 = self.d6(torch.cat([d5, e2], 1))\n",
    "        out = self.final(torch.cat([d6, e1], 1))\n",
    "\n",
    "        if self.flag:\n",
    "            logging.info(f\"Generator output: {out.shape}\")\n",
    "\n",
    "        return self.tanh(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0d6471",
   "metadata": {
    "id": "7a0d6471"
   },
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# PatchGAN Discriminator (Conditional)\n",
    "# --------------------\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=2, global_flag=False):\n",
    "        \"\"\"\n",
    "        PatchGAN Discriminator for conditional GAN\n",
    "        Args:\n",
    "            in_channels: 2 (input_image + output_image concatenated)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.flag = global_flag\n",
    "\n",
    "        # PatchGAN: C64-C128-C256-C512\n",
    "        # Note: Using InstanceNorm2d instead of BatchNorm2d for better image translation\n",
    "        self.model = nn.Sequential(\n",
    "            # C64: No normalization on first layer\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # C128\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # C256\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # C512\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # C512 (additional layer)\n",
    "            nn.Conv2d(512, 512, kernel_size=4, stride=1, padding=1),\n",
    "            nn.InstanceNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # Output: 1-channel patch predictions\n",
    "            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.flag:\n",
    "            logging.info(f\"Discriminator input: {x.shape}\")\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e156c455",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e156c455",
    "outputId": "df93d655-b4a0-43ff-9daa-8f03fb98ce85"
   },
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# Init Models, Loss & Optimizers\n",
    "# --------------------\n",
    "import os\n",
    "os.makedirs('checkpoints', exist_ok=True)\n",
    "\n",
    "# Initialize models\n",
    "G = Generator(in_channels=1, out_channels=1).to(device)\n",
    "D = Discriminator(in_channels=2).to(device)  # 2 channels: input + output\n",
    "\n",
    "# Loss functions\n",
    "adversarial_loss = nn.BCELoss()\n",
    "l1_loss = nn.L1Loss()\n",
    "\n",
    "# Optimizers with slightly different learning rates\n",
    "opt_G = optim.Adam(G.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "opt_D = optim.Adam(D.parameters(), lr=lr * 0.5, betas=(0.5, 0.999))  # D learns slower\n",
    "\n",
    "# IMPROVED: Gentler learning rate schedulers\n",
    "scheduler_G = optim.lr_scheduler.StepLR(opt_G, step_size=100, gamma=0.5)  # Changed from 50\n",
    "scheduler_D = optim.lr_scheduler.StepLR(opt_D, step_size=100, gamma=0.5)  # Changed from 50\n",
    "\n",
    "print(f\"Generator parameters: {sum(p.numel() for p in G.parameters()):,}\")\n",
    "print(f\"Discriminator parameters: {sum(p.numel() for p in D.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80dcb64",
   "metadata": {
    "id": "a80dcb64"
   },
   "outputs": [],
   "source": [
    "def plot_ssim_changes(ssim_values, title=\"SSIM changes\"):\n",
    "    \"\"\"\n",
    "    ssim_values: list or numpy array of SSIM values\n",
    "    \"\"\"\n",
    "    ssim_values = np.array(ssim_values)\n",
    "\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(ssim_values, marker='o')\n",
    "    plt.xlabel(\"Image index\")\n",
    "    plt.ylabel(\"SSIM\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d76ee5e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6d76ee5e",
    "outputId": "c921f56b-4f93-4985-dd78-fbf2572aeb03"
   },
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# TRAINING LOOP (Conditional GAN) ✅\n",
    "# --------------------\n",
    "import time\n",
    "\n",
    "ssim_values = []\n",
    "g_losses = []\n",
    "d_losses = []\n",
    "best_ssim = 0.0\n",
    "\n",
    "print(\"Starting training...\")\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    G.train()\n",
    "    D.train()\n",
    "\n",
    "    epoch_g_loss = 0.0\n",
    "    epoch_d_loss = 0.0\n",
    "\n",
    "    for batch_idx, (imgs, _) in enumerate(train_loader):\n",
    "        imgs = imgs.to(device)\n",
    "        batch_size_current = imgs.size(0)\n",
    "\n",
    "        # ======================\n",
    "        #  Train Generator\n",
    "        # ======================\n",
    "        opt_G.zero_grad()\n",
    "\n",
    "        # Generate fake images\n",
    "        gen_imgs = G(imgs)\n",
    "\n",
    "        # CRITICAL: Discriminator sees [input, output] concatenated\n",
    "        fake_pair = torch.cat([imgs, gen_imgs], dim=1)  # [B, 2, H, W]\n",
    "        pred_fake = D(fake_pair)\n",
    "\n",
    "        # Generator wants discriminator to think fake is real\n",
    "        # Use label smoothing if enabled\n",
    "        if use_label_smoothing:\n",
    "            valid = torch.ones_like(pred_fake) * real_label_value\n",
    "        else:\n",
    "            valid = torch.ones_like(pred_fake)\n",
    "\n",
    "        g_adv_loss = adversarial_loss(pred_fake, valid)\n",
    "\n",
    "        # L1 reconstruction loss\n",
    "        g_l1_loss = l1_loss(gen_imgs, imgs)\n",
    "\n",
    "        # Total generator loss\n",
    "        loss_G = lambda_G * g_adv_loss + lambda_L1 * g_l1_loss\n",
    "\n",
    "        loss_G.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(G.parameters(), max_norm=5.0)  # Gradient clipping\n",
    "        opt_G.step()\n",
    "\n",
    "        epoch_g_loss += loss_G.item()\n",
    "\n",
    "        # ======================\n",
    "        #  Train Discriminator\n",
    "        # ======================\n",
    "        opt_D.zero_grad()\n",
    "\n",
    "        # Real pairs: [input, input] (for autoencoder) or [input, target]\n",
    "        real_pair = torch.cat([imgs, imgs], dim=1)  # [B, 2, H, W]\n",
    "        pred_real = D(real_pair)\n",
    "\n",
    "        # Use label smoothing if enabled\n",
    "        if use_label_smoothing:\n",
    "            real_labels = torch.ones_like(pred_real) * real_label_value\n",
    "        else:\n",
    "            real_labels = torch.ones_like(pred_real)\n",
    "\n",
    "        real_loss = adversarial_loss(pred_real, real_labels)\n",
    "\n",
    "        # Fake pairs: [input, generated]\n",
    "        fake_pair = torch.cat([imgs, gen_imgs.detach()], dim=1)\n",
    "        pred_fake = D(fake_pair)\n",
    "\n",
    "        if use_label_smoothing:\n",
    "            fake_labels = torch.ones_like(pred_fake) * fake_label_value\n",
    "        else:\n",
    "            fake_labels = torch.zeros_like(pred_fake)\n",
    "\n",
    "        fake_loss = adversarial_loss(pred_fake, fake_labels)\n",
    "\n",
    "        # Total discriminator loss\n",
    "        d_loss = 0.5 * (real_loss + fake_loss)\n",
    "\n",
    "        d_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(D.parameters(), max_norm=5.0)  # Gradient clipping\n",
    "        opt_D.step()\n",
    "\n",
    "        epoch_d_loss += d_loss.item()\n",
    "\n",
    "    # Update learning rates\n",
    "    scheduler_G.step()\n",
    "    scheduler_D.step()\n",
    "\n",
    "    # Average losses\n",
    "    avg_g_loss = epoch_g_loss / len(train_loader)\n",
    "    avg_d_loss = epoch_d_loss / len(train_loader)\n",
    "    g_losses.append(avg_g_loss)\n",
    "    d_losses.append(avg_d_loss)\n",
    "\n",
    "    # ======================\n",
    "    #  Validation\n",
    "    # ======================\n",
    "    G.eval()\n",
    "    mse_val, ssim_epoch, n = 0.0, 0.0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, _ in val_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            gen = G(imgs)\n",
    "\n",
    "            # Convert to numpy for metrics: [-1,1] → [0,1]\n",
    "            gen_np = ((gen.cpu().numpy() + 1) / 2).clip(0, 1)\n",
    "            real_np = ((imgs.cpu().numpy() + 1) / 2).clip(0, 1)\n",
    "\n",
    "            for i in range(gen_np.shape[0]):\n",
    "                # For grayscale: [1, H, W] → [H, W]\n",
    "                gen_img = gen_np[i, 0]\n",
    "                real_img = real_np[i, 0]\n",
    "\n",
    "                ssim_epoch += ssim(real_img, gen_img, data_range=1.0)\n",
    "                n += 1\n",
    "\n",
    "    ssim_epoch /= n\n",
    "    ssim_values.append(ssim_epoch)\n",
    "\n",
    "    # ======================\n",
    "    #  Check for collapse\n",
    "    # ======================\n",
    "    if epoch > 10 and avg_d_loss < 0.01:\n",
    "        print(f\"⚠️  Warning: Discriminator too strong (D loss: {avg_d_loss:.4f})\")\n",
    "\n",
    "    # ======================\n",
    "    #  Save best model\n",
    "    # ======================\n",
    "    if ssim_epoch > best_ssim:\n",
    "        best_ssim = ssim_epoch\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'G_state_dict': G.state_dict(),\n",
    "            'D_state_dict': D.state_dict(),\n",
    "            'opt_G_state_dict': opt_G.state_dict(),\n",
    "            'opt_D_state_dict': opt_D.state_dict(),\n",
    "            'best_ssim': best_ssim,\n",
    "        }, 'checkpoints/best_model.pth')\n",
    "\n",
    "    # ======================\n",
    "    #  Save checkpoint periodically\n",
    "    # ======================\n",
    "    if (epoch + 1) % save_interval == 0:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'G_state_dict': G.state_dict(),\n",
    "            'D_state_dict': D.state_dict(),\n",
    "            'opt_G_state_dict': opt_G.state_dict(),\n",
    "            'opt_D_state_dict': opt_D.state_dict(),\n",
    "            'ssim_values': ssim_values,\n",
    "            'g_losses': g_losses,\n",
    "            'd_losses': d_losses,\n",
    "        }, f'checkpoints/checkpoint_epoch_{epoch+1}.pth')\n",
    "\n",
    "    # ======================\n",
    "    #  Print progress\n",
    "    # ======================\n",
    "    elapsed = time.time() - start_time\n",
    "    print(\n",
    "        f\"Epoch [{epoch+1:3d}/{epochs}] | \"\n",
    "        f\"Time: {elapsed/60:.1f}min | \"\n",
    "        f\"G: {avg_g_loss:.4f} | \"\n",
    "        f\"D: {avg_d_loss:.4f} | \"\n",
    "        f\"SSIM: {ssim_epoch:.4f} | \"\n",
    "        f\"Best SSIM: {best_ssim:.4f}\"\n",
    "    )\n",
    "\n",
    "print(f\"\\n✅ Training completed in {elapsed/60:.1f} minutes\")\n",
    "print(f\"Best SSIM: {best_ssim:.4f}\")\n",
    "\n",
    "# Plot training metrics\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].plot(g_losses, label='Generator')\n",
    "axes[0].plot(d_losses, label='Discriminator')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Losses')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "axes[1].plot(ssim_values, marker='o')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('SSIM')\n",
    "axes[1].set_title('Validation SSIM')\n",
    "axes[1].grid(True)\n",
    "\n",
    "axes[2].plot([opt_G.param_groups[0]['lr']] * len(g_losses))\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('Learning Rate')\n",
    "axes[2].set_title('Learning Rate Schedule')\n",
    "axes[2].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('checkpoints/training_curves.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e009bb3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 514
    },
    "id": "0e009bb3",
    "outputId": "f78028be-2feb-4acc-a7a9-038da5b8eeab"
   },
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# Generate & Show Images\n",
    "# --------------------\n",
    "def show_results(model, loader, num_images=5):\n",
    "    \"\"\"Visualize input vs generated images\"\"\"\n",
    "    model.eval()\n",
    "    imgs, _ = next(iter(loader))\n",
    "    imgs = imgs[:num_images].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        gen = model(imgs)\n",
    "\n",
    "    # Denormalize: [-1,1] → [0,1]\n",
    "    imgs = (imgs.cpu() + 1) / 2\n",
    "    gen = (gen.cpu() + 1) / 2\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    for i in range(num_images):\n",
    "        # Input images\n",
    "        plt.subplot(2, num_images, i+1)\n",
    "        plt.imshow(imgs[i, 0], cmap='gray')\n",
    "        plt.title('Input' if i == 2 else '')\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        # Generated images\n",
    "        plt.subplot(2, num_images, i+1+num_images)\n",
    "        plt.imshow(gen[i, 0], cmap='gray')\n",
    "        plt.title('Generated' if i == 2 else '')\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('checkpoints/generated_samples.png', dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "show_results(G, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8b8edb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 635
    },
    "id": "da8b8edb",
    "outputId": "06df4495-7ada-48fb-c266-0302af83c3fc"
   },
   "outputs": [],
   "source": [
    "def generate_and_evaluate_ssim(generator, dataloader, device, num_images=5):\n",
    "    \"\"\"\n",
    "    Generate images and evaluate SSIM with visualization\n",
    "    \"\"\"\n",
    "    generator.eval()\n",
    "\n",
    "    # Get real images\n",
    "    real_imgs, _ = next(iter(dataloader))\n",
    "    real_imgs = real_imgs[:num_images].to(device)\n",
    "\n",
    "    # Generate images\n",
    "    with torch.no_grad():\n",
    "        gen_imgs = generator(real_imgs)\n",
    "\n",
    "    # Convert to numpy and denormalize: [-1,1] → [0,1]\n",
    "    real_np = ((real_imgs.cpu().numpy() + 1) / 2).clip(0, 1)\n",
    "    gen_np = ((gen_imgs.cpu().numpy() + 1) / 2).clip(0, 1)\n",
    "\n",
    "    ssim_scores = []\n",
    "\n",
    "    # Plot comparison\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    for i in range(num_images):\n",
    "        # Real images\n",
    "        plt.subplot(2, num_images, i+1)\n",
    "        plt.imshow(real_np[i, 0], cmap=\"gray\", vmin=0, vmax=1)\n",
    "        plt.title(\"Input\" if i == 2 else \"\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        # Generated images\n",
    "        plt.subplot(2, num_images, i+1+num_images)\n",
    "        plt.imshow(gen_np[i, 0], cmap=\"gray\", vmin=0, vmax=1)\n",
    "\n",
    "        # Calculate SSIM\n",
    "        score = ssim(real_np[i, 0], gen_np[i, 0], data_range=1.0)\n",
    "        ssim_scores.append(score)\n",
    "        plt.title(f\"SSIM: {score:.3f}\" if i == 2 else f\"{score:.3f}\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Average SSIM: {np.mean(ssim_scores):.4f}\")\n",
    "    print(f\"SSIM scores: {[f'{s:.4f}' for s in ssim_scores]}\")\n",
    "\n",
    "    return ssim_scores\n",
    "\n",
    "# Evaluate on validation set\n",
    "generate_and_evaluate_ssim(G, val_loader, device, num_images=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbf8e15",
   "metadata": {
    "id": "ebbf8e15"
   },
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# Load Best Model (Optional)\n",
    "# --------------------\n",
    "def load_checkpoint(checkpoint_path):\n",
    "    \"\"\"Load a saved checkpoint\"\"\"\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    G.load_state_dict(checkpoint['G_state_dict'])\n",
    "    D.load_state_dict(checkpoint['D_state_dict'])\n",
    "    print(f\"✅ Loaded checkpoint from epoch {checkpoint['epoch']}\")\n",
    "    if 'best_ssim' in checkpoint:\n",
    "        print(f\"   Best SSIM: {checkpoint['best_ssim']:.4f}\")\n",
    "    return checkpoint\n",
    "\n",
    "# Example: Load best model\n",
    "# checkpoint = load_checkpoint('checkpoints/best_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b09fd3",
   "metadata": {
    "id": "06b09fd3"
   },
   "outputs": [],
   "source": [
    "real_imgs, _ = next(iter(train_loader))\n",
    "real_imgs = real_imgs[:1].to(device)\n",
    "gen_imgs = G(real_imgs)\n",
    "\n",
    "real_imgs = (real_imgs + 1) / 2    #[-1,+1]->[0,1]\n",
    "gen_imgs = (gen_imgs + 1) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6aae99",
   "metadata": {
    "id": "aa6aae99"
   },
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# NMSE: Normalized MSE ↓\n",
    "# --------------------\n",
    "def NMSE(x, y):        # x, y: [B, C, H, W], values [0,1]\n",
    "    mse = F.mse_loss(x, y, reduction='none')\n",
    "    nmse = mse.view(mse.size(0), -1).sum(dim=1) / (y.view(y.size(0), -1)**2).sum(dim=1)\n",
    "    return nmse.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb6e12a",
   "metadata": {
    "id": "bcb6e12a"
   },
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# NMAE: Normalized MAE ↓\n",
    "# --------------------\n",
    "def NMAE(x, y, eps=1e-8):\n",
    "    num = torch.abs(x - y).view(x.size(0), -1).sum(dim=1)\n",
    "    den = torch.abs(y).view(y.size(0), -1).sum(dim=1) + eps\n",
    "    return (num / den).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8add1a3",
   "metadata": {
    "id": "b8add1a3"
   },
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# PSNR ↑\n",
    "# --------------------\n",
    "def PSNR(x, y, max_val=1.0):\n",
    "    x_np = x.detach().cpu().numpy()\n",
    "    y_np = y.detach().cpu().numpy()\n",
    "    psnr_vals = [psnr(y_np[i], x_np[i], data_range=max_val) for i in range(x_np.shape[0])]\n",
    "    return np.mean(psnr_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3891b893",
   "metadata": {
    "id": "3891b893"
   },
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# SSIM ↑\n",
    "# --------------------\n",
    "def SSIM(x, y):\n",
    "    x_np = x.detach().cpu().numpy().astype(np.float32)\n",
    "    y_np = y.detach().cpu().numpy().astype(np.float32)\n",
    "\n",
    "    ssim_vals = []\n",
    "    for i in range(x_np.shape[0]):\n",
    "        # Convert from [C,H,W] to [H,W,C]\n",
    "        ssim_val = ssim(\n",
    "            y_np[i].transpose(1,2,0),\n",
    "            x_np[i].transpose(1,2,0),\n",
    "            channel_axis=2,\n",
    "            data_range=1.0\n",
    "        )\n",
    "        ssim_vals.append(ssim_val)\n",
    "\n",
    "    return np.mean(ssim_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d106edca",
   "metadata": {
    "id": "d106edca"
   },
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# MS-SSIM ↓ (1 - ms-ssim)\n",
    "# --------------------\n",
    "def MS_SSIM(x, y):\n",
    "    return 1 - ms_ssim(x, y, data_range=1.0, size_average=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cfbb24",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c8cfbb24",
    "outputId": "5788be74-2c17-42b5-f1b0-cc25f1c250b4"
   },
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# LPIPS ↓\n",
    "# --------------------\n",
    "\n",
    "lpips_alex = lpips.LPIPS(net='alex').to(device)\n",
    "def LPIPS(x, y):\n",
    "    return lpips_alex(x*2-1, y*2-1).mean().item()  # scale to [-1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8f780d",
   "metadata": {
    "id": "4d8f780d"
   },
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# FID ↓\n",
    "# --------------------\n",
    "def FID(x, y, device=None, eps=1e-6):\n",
    "    device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    if x.shape[0] < 2 or y.shape[0] < 2:\n",
    "        raise ValueError(\"FID requires at least 2 images\")\n",
    "\n",
    "    weights = Inception_V3_Weights.DEFAULT\n",
    "    inception = inception_v3(weights=weights, aux_logits=False)\n",
    "    inception.fc = torch.nn.Identity()\n",
    "    inception.to(device).eval()\n",
    "\n",
    "    def preprocess(imgs):\n",
    "        if imgs.shape[1] == 1:\n",
    "            imgs = imgs.repeat(1, 3, 1, 1)\n",
    "        elif imgs.shape[1] == 2:\n",
    "            imgs = torch.cat([imgs, imgs[:, :1]], dim=1)\n",
    "        elif imgs.shape[1] != 3:\n",
    "            raise ValueError(\"Channels must be 1, 2, or 3\")\n",
    "        return F.interpolate(imgs, size=(299, 299), mode='bilinear', align_corners=False)\n",
    "\n",
    "    def get_features(imgs):\n",
    "        imgs = preprocess(imgs.to(device))\n",
    "        with torch.no_grad():\n",
    "            feats = inception(imgs)\n",
    "        feats = feats.cpu().numpy().astype(np.float64)\n",
    "\n",
    "        if not np.isfinite(feats).all():\n",
    "            raise ValueError(\"NaN or Inf detected in Inception features\")\n",
    "\n",
    "        return feats\n",
    "\n",
    "    fx = get_features(x)\n",
    "    fy = get_features(y)\n",
    "\n",
    "    mu_x, mu_y = fx.mean(0), fy.mean(0)\n",
    "    sigma_x = np.cov(fx, rowvar=False)\n",
    "    sigma_y = np.cov(fy, rowvar=False)\n",
    "\n",
    "    # Regularize covariance (CRITICAL)\n",
    "    sigma_x += np.eye(sigma_x.shape[0]) * eps\n",
    "    sigma_y += np.eye(sigma_y.shape[0]) * eps\n",
    "\n",
    "    covmean, _ = linalg.sqrtm(sigma_x @ sigma_y, disp=False)\n",
    "\n",
    "    if not np.isfinite(covmean).all():\n",
    "        covmean = linalg.sqrtm(\n",
    "            (sigma_x + sigma_y) / 2\n",
    "        )\n",
    "\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "\n",
    "    fid = np.sum((mu_x - mu_y) ** 2) + np.trace(sigma_x + sigma_y - 2 * covmean)\n",
    "    return float(fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd7980e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6fd7980e",
    "outputId": "72177c10-425f-4898-f972-2f3beb4f6d3f"
   },
   "outputs": [],
   "source": [
    "print('NMSE(↓):',NMSE(gen_imgs,real_imgs))\n",
    "print('NMAE(↓):',NMAE(gen_imgs,real_imgs))\n",
    "print('PSNR(↑):',PSNR(gen_imgs,real_imgs))\n",
    "print('SSIM(↑):',SSIM(gen_imgs,real_imgs))\n",
    "print('MS_SSIM(↑):',MS_SSIM(gen_imgs,real_imgs))\n",
    "#print('FID(↓):',FID(torch.cat(gen_imgs, dim=0),torch.cat(real_imgs, dim=0)))\n",
    "#print('LPIPS(↓):',LPIPS(gen_imgs,real_imgs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4369f6eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
